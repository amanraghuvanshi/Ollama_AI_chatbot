# DeepSeek AI Chatbot

## Overview

DeepSeek AI Chatbot is a locally hosted, context-aware chatbot powered by the `deepseek-r1:1.5b` model. Designed for privacy and offline functionality, the chatbot is built with a modular architecture, featuring a robust backend, interactive frontend (Streamlit/Gradio), and seamless integration with the Ollama platform.

---

## Features

* **Privacy-first**: Runs locally, ensuring data security.
* **Customizable Frontend**: Choose between Streamlit or Gradio interfaces.
* **Modular Design**: Backend and frontend components are decoupled for scalability and ease of maintenance.
* **AI-Powered**: Leverages the advanced capabilities of the `deepseek-r1:1.5b` model.

---

## System Requirements

* **Hardware:**

  * Minimum 8 GB RAM (16 GB recommended)
  * 4 CPU cores
  * 10 GB free disk space

* **Software:**

  * Python 3.8+
  * Pip
  * Ollama CLI (installed and configured)
  * Supported OS:

    * Windows 10+
    * macOS 10.15+
    * Linux (Ubuntu 20.04 or later)

---

## Installation

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd project-root
```

### Step 2: Set Up Python Environment

1. Create a virtual environment:

   ```bash
   python -m venv venv
   ```
2. Activate the virtual environment:

   * **Windows:**

     ```bash
     venv\Scripts\activate
     ```
   * **Mac/Linux:**

     ```bash
     source venv/bin/activate
     ```
3. Install dependencies:

   ```bash
   pip install -r backend/requirements.txt
   pip install -r frontend/requirements.txt
   ```

### Step 3: Install and Configure Ollama

1. Install the Ollama CLI:

   * Follow [Ollama's installation guide](https://ollama.com/docs/installation).
2. Pull the `deepseek-r1:1.5b` model:

   ```bash
   ollama pull deepseek-r1:1.5b
   ```

### Step 4: Configure Environment Variables

1. Create a `.env` file in the root directory:

   ```plaintext
   OLLAMA_PATH=/path/to/ollama
   ```
2. Adjust the `OLLAMA_PATH` to point to your local Ollama installation.

---

## Running the Application

### Backend

Start the backend server:

```bash
python backend/app.py
```

The server runs on:

```
http://localhost:8000
```

### Frontend

#### Option 1: Streamlit

1. Run the Streamlit application:

   ```bash
   streamlit run frontend/streamlit_app.py
   ```
2. Open the displayed URL (e.g., `http://localhost:8501`).

#### Option 2: Gradio

1. Run the Gradio application:

   ```bash
   python frontend/gradio_app.py
   ```
2. Open the displayed URL (e.g., `http://localhost:7860`).

---

## Usage

1. Enter your query in the frontend interface.
2. Submit the query.
3. View the response generated by the AI model.

### Example Queries

* **General:** "What is artificial intelligence?"
* **Technical:** "Explain the difference between a neural network and a decision tree."
* **Creative:** "Write a short story about a robot exploring space."

---

## Project Structure

```plaintext
project-root/
├── backend/                # Backend logic
├── frontend/               # Frontend logic
├── models/                 # Model-related files
├── data/                   # Data files
├── tests/                  # Unit and integration tests
├── docs/                   # Documentation files
├── logs/                   # Logs for debugging
├── .env                    # Environment variables
├── README.md               # Main project documentation
├── LICENSE                 # License information
└── setup.py                # Python package setup
```

---

## Troubleshooting

### Common Issues

1. **Model Not Found:**

   * Ensure the `deepseek-r1:1.5b` model is pulled using the Ollama CLI.
   * Verify the `OLLAMA_PATH` in the `.env` file.

2. **Backend Not Responding:**

   * Ensure the backend server is running (`python backend/app.py`).
   * Check if another service is using port 8000.

3. **Frontend Not Loading:**

   * Ensure dependencies are correctly installed.
   * Restart the frontend application.

### Logs

* Application logs can be found in the `logs/` directory:

  * `app.log` for general logs.
  * `errors.log` for error-specific logs.

---

## Future Enhancements

* Multi-language support.
* Integration with additional AI models.
* Enhanced visualization and UI improvements.

---

## License

This project is licensed under the [MIT License](LICENSE).

---

## Contact

* **Email:** [support@example.com](mailto:support@example.com)
* **GitHub Issues:** [Project Repository](https://github.com/your-repository-link)

Thank you for using the DeepSeek AI Chatbot!
